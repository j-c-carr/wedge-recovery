import re
import sys
import h5py
import logging

import os
import typing
from typing import Optional, List
import numpy as np
import tensorflow as tf
from skimage.transform import resize
from sklearn.preprocessing import normalize

def init_logger(f: str, 
                name: str) -> logging.Logger:
    """Instantiates logger :name: and sets logfile to :f:"""
    logger = logging.getLogger(name)

    logger.setLevel(logging.INFO)
    formatter = logging.Formatter("%(asctime)s: %(levelname).1s %(filename)s:%(lineno)d] %(message)s")
    file_handler = logging.FileHandler(f)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    return logger

logger = init_logger("test.log", __name__)

class UtilManager:
    """Manager class for miscellaneous I/O operations"""

    def __init__(self):
        self.dset_attrs = {}
        self.random_seeds = {}
        self.data = {}
        self.metadata = {}


    def load_data_from_h5(self, filepath):
        """Loads all data from h5 file, returns nothing. (Typically used just
        to observe the values in a dataset)"""
        self.filepath = filepath

        with h5py.File(filepath, "r") as hf:

            for k in hf.keys():

                # AstroParams are stored in h5py groups
                if isinstance(hf[k], h5py.Group):
                    logger.debug(f"{k} is a group")
                    self.metadata[k] = {}
                    for k2 in hf[k].keys():
                        v = np.array(hf[k][k2], dtype=np.float32)
                        logger.debug(f"\t{k2} created in metadata.")
                        self.metadata[k][k2] = v

                # Lightcones are stored as h5py datasets
                if isinstance(hf[k], h5py.Dataset):
                    v = np.array(hf[k][:], dtype=np.float32)
                    assert np.isnan(np.sum(v)) == False
                    self.data[k] = v
            self.data["redshifts"].reshape(-1) 

            # Load metadata from h5 file
            for k, v in hf.attrs.items():
                self.dset_attrs[k] = v

        # Print success message
        print("\n----------\n")
        print(f"data loaded from {self.filepath}")
        print("Contents:")
        for k, v in self.data.items():
            print("\t{}, shape: {}".format(k, v.shape))
        print("\nMetadata:")
        for k in self.metadata.keys():
            print(f"\t{k}")
        print("\n----------\n")
        print("\nDataset Attributes:")
        for k in self.dset_attrs.keys():
            print(f"\t{k}")
        print("\n----------\n")


    def save_results(self, 
                     filename: str, 
                     results: dict,
                     start: int,
                     end: int) -> None:
        """
        Saves the model predictions (along original data) to an h5 file. By
        specifying a [start:end] range, we save the original lightcones
        and wedge_filtered lightcones that were used to produce the results.
        -----
        Params:
        :filename: filename of output file
        :results:  key is dataset name, value is an np.ndarray of shape
                   (num_saved_samples, *lightcone_size)
        :start:    starting index of original data to save.
        :end:      ending index of original data to save
        """

        with h5py.File(filename, "w") as hf:

            # Save all results generated by model
            for dset_name, _data in results.items():
                assert end - start == _data.shape[0], \
                        f"end-start must match results shape."

                logger.info(f"Saving {dset_name} dataset.")
                hf.create_dataset(dset_name, data=_data)

            # Save the data that was used to generate the results
            for dset_name, _data in self.data.items():
                assert 0<= start and end <= _data.shape[0], \
                        f"end-start must match _data shape"

                logger.info(f"Saving {dset_name} as a dataset.")
                hf.create_dataset(dset_name, data=_data[start:end])

            # Save the corresponding meta data
            for grp_name, grp_data in self.metadata.items():
                grp = hf.create_group(grp_name)
                logger.info(f"Saving {grp_name} as a group.")

                for dset_name, _data in grp_data.items():
                    assert 0<= start and end <= _data.shape[0], \
                            f"error: end-start must match _data shape"
                    logger.info(f"\tSaving {dset_name} values.")
                    grp.create_dataset(dset_name, data=_data[start:end])

            # Save the original dataset attributes
            for k, v in self.dset_attrs.items():
                logger.info(f"Saving {k} attribute.")
                hf.attrs[k] = str(v)


    def binarize_ground_truth(self, Y: np.ndarray) -> np.ndarray:
        mins = Y.min(axis=(2,3), keepdims=True)
        return (Y-mins > 0).astype(np.float32)


    def write_str(self, 
                  s: str, 
                  filename: str) -> None:
        """Writes string to file"""
        assert type(s) is str

        with open(filename,"w") as f:
            f.write(s)
            f.close()

